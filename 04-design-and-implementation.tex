\chapter{Design and Implementation}\label{ch:designimplementation}

\section{Emulator implementation}
Both the register and stack emulators (plus the conversion ``emulator'') will be
written to be part of the same program, with the different functionality
controlled with commandline flags. This method enables the emulators to share
functionality where appropriate, especially for the conversion `machine' which
can be built directly on top of the stack machine using object orientated
programming techniques.

Early in the implementation, it was decided to cut down on the instructions that
the emulators, as they were either esoteric or difficult to implement while
being irrelevant to this project. In particular, none of the special opcodes for
the DCPU-16 (see Appendix~\ref{ch:dcpuspec}) were implemented, and neither were
the \lstinline{PUSH} or \lstinline{POP} identifiers, as they are used to
implement a `reverse stack' starting at the top of the memory, 0xffff. Since
this project tests converting register code to stack code, it doesn't make much
sense to have most of a stack in the register architecture. This does exclude
the possibility of having a call stack, with procedures or functions, but this
is an acceptable compromise as they can be emulated by branching instructions
instead.

Similarly, the J5 (see Appendix~\ref{ch:j5spec}) includes \lstinline{CALL nn} \&
\lstinline{RETURN} instructions, so for feature parity it was decided to exclude
these as well. The J5 also has a \lstinline{SSET n} function, which is intended
as a fast version of \lstinline{SET nn} for small numbers. Since this is an
emulator, this can be replaced with \lstinline{SET} quite trivially. However, if
it were included, it would be a good target for optimisation, as since the
operand is constant, it would be trivial to replace the \lstinline{SET}
instruction with \lstinline{SSET} where appropriate.

\subsection{Algorithms used}
The general structure for both emulators is fairly similar. First the input
file is read into memory. It is then iterated over, line-by-line, and tokenised
into an intermediary representation, which serves to validate the input syntax.
This can be approximated with the following pseudocode for the register
architecture:

\begin{lstlisting}[caption={Tokenising algorithm for the DCPU-16}]
function tokenise(filepath: string): instruction
  load file from filepath
  For line in file do
    instruction;
    split line into words # words separated by whitespace
    word = words[0]
    If word is a label # is prefixed by a ':'
      instruction.label = first word
      word++
    If word is not an opcode
      return error "not a valid instruction"
    instruction.code = word
    word++

    # opcodes take a variable number of operands
    If number of operands for instruction.code != remaining_words
      return error "invalid instruction" + instruction.opcode
    instruction.operands = remaining words
    return instruction
  end
\end{lstlisting}

It was quite difficult to determine the type of the operands from the source.
C++ in particular made it difficult due to its type system. The eventual
solution was to use a variant (typesafe union) type to store integers (for
literals), register references, and strings for everything else. This
necessitated the use of a third-party library, Boost, as C++ does not have such
a type in its standard library, although it does gain one in C++17, the latest
standard that as of 2017 is not yet fully included in a stable release of any of
the common compilers. Boost is a large and complicated library, but if usage is
limited to only required features it won't increase maintenance costs
significantly.  Using strings for the ``everything else'' part of the operand
proved to be quite complicated as it included not only labels and string
literals, but memory address lookups as well (e.g.  \lstinline{SET [2000], 42}).
For executing purposes it was possible to check if the string started and ended
with \texttt{\rbrack} and \texttt{\rbrack} respectively, and getting the value
of the operand inside. This was further complicated as the DCPU-16 has the
ability to use compound expressions inside the memory address brackets, e.g.
\lstinline{[2000+I]} where the contents of the `I' register is added to 2000 to
get the memory address referred to. A similar solution was made for the stack
emulator.

For running the emulators themselves, they provide a \lstinline{run} function,
which is passed the tokenised program to run (along with any flags such as
verbosity of output). It then initialises the machine's state and runs through
each instruction in the program until terminated or it reaches the end.

\begin{lstlisting}[caption={Running a DCPU-16 program}]
procedure run(program: Program, flags..)
  set registers to 0
  program_counter = 0
  while program_counter < sizeof(program)
    run_instruction(program[program_counter])
    program_counter++
  end
\end{lstlisting}

The above is a simple view of the actual function though, which has the ability
to run at a certain clock speed (to mimic the speed the actual processor would
process instructions) or just as fast as the emulator can run, which is more
useful for testing purposes. There's also a verbosity flag which dumps the state
of the machine, for debugging purposes.

For each parsed instruction, a member function for the class of the relevant
emulator was written. To link each of these methods to their respective
instruction, it seemed best to use a \texttt{std::map} as provided by the
standard library. In many places, the functions were very short, only consisting
of a single line of code, so it was decided to make use of C++'s lambdas. This
meant, for example, that the implementation for the J5's \texttt{INC}
instruction only consisted of
\verb|[](machine *m){m->stack.top()++;}| instead of just referring to the
function pointer \verb|&machine::inc_func| in the class' function map, which
avoids all the extra code overhead of having to fully declare an extra method.
Where the instruction implementation functions were beyond a line, the usual
style was kept to help keep the code readable. Taking this further, it was
noticed that many of the instructions followed a pattern (for example,
\texttt{ADD} and \texttt{SUB} from the J5 both operate on the top two items on
the stack). As the code for these implementations would've otherwise been
duplicated several times (where only the exact operation differs), a helper
function was created to deal with these. This resulted in
\verb|[](machine *m){m->binop_func([](int a, int b){return a+b;});}| which even
though works as expected, it is decidedly unreadable for anyone looking through
the code for themselves.

The function loop also has to deal with the effects of the conditional
expressions. For the DCPU-16 this is the \lstinline{IFx} statements where they
only perform the next expression if the condition is true, whereas the the J5
has some test instructions (e.g. \lstinline{TEQ}) and \lstinline{BRZERO}
instruction which conditionally branches on whether the test instruction set the
appropriate flag. The notable thing here is that the test instruction can be
executed long before the branch is actually taken, which allows for more freedom
over branching. It also has \lstinline{BRANCH} for unconditionally branching.

\section{Conversion routines}
Initially, the conversion was done all at once --- the program would take in a
register program and output a stack program that could be run by the stack
interpreter. This served well as an initial step, but to get to the point of
caching blocks of code, the implementation had to be changed first to converting
one instruction at a time and then executing it as the stack machine all at
once, to converting blocks of instructions and executing them. The process for
this can be seen in Listing~\ref{lst:convertcache}

\begin{lstlisting}[label={lst:convertcache},caption={Converted instruction
caching},float]
cache: map positions to stack code;
function convert_instructions(start_pos: instruction)
  If start_pos not in cache
    last_pos = find next label
    cache[start_pos] = convert_instructions(start_pos, last_pos)
  return cache[start_pos]
\end{lstlisting}

Table~\ref{tab:conversionexs} is a table that shows some examples of the stack
code produced for some register instructions. By way of explanation, for the
\lstinline{ADD A, B} row the code snippet first loads register A (represented by
8191) onto the stack, loading register B (8190) onto the stack, adding them
together, and storing the result back into A. The DCPU-16's \lstinline{IFx}
statements are more complex to convert as the J5 does not have a direct
equivalent. The \lstinline{IFx} instruction only executes the next instruction
if the condition (e.g. \lstinline{B != A} for \lstinline{IFN B, A}) evaluates to
true. For the J5, the only conditional branching mechanism is \lstinline{BRZERO}
and a series of `test' instructions, that have to be executed before hand and
set a flag to signify whether its test is true or not. So, the conversion
routine needs to load the operands for the statement onto the stack, test the
operands on the stack, the drop the operands (so not to leave them ``dangling''
on the stack regardless of the condition result), then \lstinline{BRZERO} with
an operand that's equal to the generated length of the next instruction.  With
regard to implementation, it was necessary to leave a temporary value for the
operand initially, then go back and change it once the next instruction had been
converted.

The implementation of this is particularly nice as the functions which load the
instruction operands onto the stack (as in, e.g., \lstinline{ADD B, A}) were
able to use one another, with minimal code duplication. For example, loading a
register value onto the stack is just the same as loading an address onto the
stack but with an extra \lstinline{LOAD} instruction following it. These segment
functions allowed operands to be loaded onto the stack with no additional
side effects. A complicated example of this can be seen with the
\lstinline{SET A, [3000+I]} in Table~\ref{tab:conversionexs} that uses the
DCPU-16's ability to have arithmetic operations inside memory addresses.

\begin{table}
\caption{Instruction conversion examples}
\begin{tabular}{l l}\label{tab:conversionexs}
Register instruction & Converted stack code \\ \toprule
\begin{lstlisting}
SET A, 5000
\end{lstlisting} &
\begin{lstlisting}
SET 5000
SET 8191
STORE
\end{lstlisting} \\ \midrule
\begin{lstlisting}
ADD A, B
\end{lstlisting} &
\begin{lstlisting}
SET 8191
LOAD
SET 8190
LOAD
ADD
SET 8191
STORE
\end{lstlisting} \\ \midrule
\begin{lstlisting}
SET PC, LOOP
\end{lstlisting} &
\begin{lstlisting}
BRANCH LOOP
\end{lstlisting} \\ \midrule
\begin{lstlisting}
IFN A, 0
SET PC, LOOP
\end{lstlisting} &
\begin{lstlisting}
SET 8191
LOAD
SET 0
TEQ
DROP
DROP
BRZERO 2
BRANCH LOOP
\end{lstlisting} \\ \midrule
\begin{lstlisting}
SET A, [3000+I]
\end{lstlisting} &
\begin{lstlisting}
SET 3000
SET 8185
LOAD
ADD
LOAD
SET 8191
STORE
\end{lstlisting} \\
\end{tabular}
\end{table}


\section{Optimisation}
Optimisation was done on the stack code generated by blocks of register
instructions.

\subsection{Peephole optimisation}
A number of peephole optimisations were implemented as part of the optimisation
process. A list of them can be seen in Table~\ref{tab:peepholeex}. While it has
some effect on its own, typically around the `edges' of generated stack
code, it tends to have greater effect following a stack scheduling pass.

The optimiser iterates over the program (which is again separated into regions
by label) in blocks of a certain size that's determined by the individual
optimisation. When it finds a block that matches the pattern that it's looking
for, it replaces that block as defined by the optimisation.

% Oh dear.
\begin{table}
\caption{Peephole optimisation examples}
\begin{tabularx}{\linewidth}{l l X}\label{tab:peepholeex}
Original stack code & Optimised stack code & Notes \\ \toprule
\begin{lstlisting}^^J
SET 1^^J
ADD
\end{lstlisting} &
\begin{lstlisting}^^J
INC
\end{lstlisting} &
Take advantage of the J5's {\lstinline!INC!} instruction \\ \midrule
\begin{lstlisting}^^J
SET 1^^J
SUB
\end{lstlisting} &
\begin{lstlisting}^^J
DEC
\end{lstlisting} &
Same as above, but for subtraction \\ \midrule
\begin{lstlisting}^^J
SET 0^^J
TEQ^^J
DROP
\end{lstlisting} &
\begin{lstlisting}^^J
TSZ
\end{lstlisting} &
Generated with an {\lstinline!IFN, x, 0!} statement. Instead take advantage of
the {\lstinline!TSZ!} instruction of the J5 \\ \midrule
\begin{lstlisting}^^J
SET x^^J
STORE^^J
SET x^^J
LOAD
\end{lstlisting} &
\begin{lstlisting}^^J
DUP^^J
SET x^^J
STORE
\end{lstlisting} &
A store followed by an immediate load of the same value. Instead, duplicate the
stored value and just store (since it might be used elsewhere) \\ \midrule
\begin{lstlisting}^^J
DUP^^J
SWAP
\end{lstlisting} &
\begin{lstlisting}^^J
DUP
\end{lstlisting} &
Removes redundant swap \\ \midrule
\begin{lstlisting}^^J
SWAP^^J
SWAP
\end{lstlisting} &
\begin{lstlisting}^^J
<NOP>
\end{lstlisting} &
Removes redundant swaps \\ \midrule
\begin{lstlisting}^^J
SET x^^J
DROP
\end{lstlisting} &
\begin{lstlisting}^^J
<NOP>
\end{lstlisting} &
Removes unused stack usage \\ \bottomrule
\end{tabularx}
\end{table}

\subsection{Stack scheduling}
Stack scheduling was simple to implement and is described in pseudocode by
Listing~\ref{lst:schedule}.

\begin{lstlisting}[caption={Stack scheduling
implementation},float,label=lst:schedule]

pairs = []

function get_stack_depth(pos)
  return number of pairs such that p.i < pos and pos < p.j

For ins = program.begin to program.end
  if ins == SET and ins + 1 == STORE
    For ins2 = ins + 2 to program.end
      if ins2 == SET and ins2 + 1 == LOAD
        # found a match
        pairs.append({ins, ins2})
        break ; don't include the same values more than once
    end
end
sort pairs by distance (j-i) # so we apply tightest loops first
For (i, j) in pairs
  stack_depth = get_stack_depth(i)
  if stack_depth > 2 or get_stack_depth(j) > 2
    continue ; too deep in the stack to bring back to the top

  if stack_depth == 0
    ins = DUP
  else if stack_depth == 1
    ins = TUCK2
  else if stack_depth == 2
    ins = TUCK3

  remove instructions at positions j and j+1
  insert ins before position i

  stack difference = net stack depth change between i and j
  if stack difference == 0
    # no op
  else if stack difference == 1
    insert SWAP after position j
  else if stack difference == 2
    insert RSD3 after position j

  For p in remaining pairs
    adjust i, j according to insertion/deletion change
  end
end
\end{lstlisting}

The key to the instruction that needs to be inserted at the ``reuse'' point to
bring the instruction to the top of the stack depends on how many values have
been added by instructions between its store point and its reuse point. This can
be determined statically as all instructions have a known behaviour (whether
they remove or add values to the stack).

An example of what this algorithm produces is listed in
Table~\ref{tab:scheduleex}. It's worth noting that even though the stack code
produced by the stack scheduling is only 3 instructions shorter, it only
contains 6 memory read/writes, compared to 10 for the initial unoptimised stack
code which is a 40\% reduction.

\begin{table}
\caption{Stack scheduling example}
\begin{tabular}{c c c}\label{tab:scheduleex}
  Original register code & Initial stack code & Stack scheduled code
  \\ \toprule
\begin{lstlisting}
; b = a + c
SET A, 23
SET C, 7
ADD X, A
ADD X, C
SET B, X
\end{lstlisting} &
\begin{lstlisting}
SET 23
SET 8191
STORE
SET 7
SET 8189
STORE
SET 8188
LOAD
SET 8191
LOAD
ADD
SET 8188
STORE
SET 8188
LOAD
SET 8189
LOAD
ADD
SET 8188
STORE
SET 8188
LOAD
SET 8190
STORE
\end{lstlisting} &
\begin{lstlisting}
SET 23
DUP
SET 8191
STORE
SET 7
TUCK2
SET 8189
STORE
SET 8188
LOAD
SWAP
ADD
DUP
SET 8188
STORE
ADD
DUP
SET 8188
STORE
SET 8190
STORE
\end{lstlisting} \\
\end{tabular}
\end{table}

Due to the na{\"\i}vity of how the blocks are determined (splitting on labels),
if a loop doesn't have any labels that segment the code that follows the end of
the loop, it will also be included in the block.  This causes issues with the
stack-scheduling optimiser as it may find pairs of STORE/LOAD which are actually
outside the block. For the purposes of this project, an (unused) label at the
actual logical end of the block.

\section{Overall structure}
The overall structure of how the translator performs is shown in
Figure~\ref{fig:implementationstructure}. The program is similar to Transmeta's
CMS structure (as seen in Figure~\ref{fig:cmsctrlflow}) with its translator and
associated cache system along with how it iterates and refers to the translated
code, but lacks the translation threshold and the ability to rollback and
interpret the instruction in the register language. While this implementation
can interpret the register code, it cannot switch between that and translating
in a single run. This diagram also shows the control flow for the optimisation
patterns which Transmeta's diagram lacks. Similarly to the Transmeta diagram, it
also doesn't show the end condition of the Interpreter/Translator combination,
when the program counter reaches the end of the program, or an unrecoverable
fault occurs.

\begin{figure}
  \begin{tikzpicture}[node distance=3cm, on grid, auto, >=stealth, thick,
                      text centered, font=\small]
    \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm,
                           draw=black, fill=white, text width=2.5cm]
    \tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm,
                            draw=black, fill=white, text width=2.5cm]

    \begin{scope}[on background layer]
      \fill[blue!60!,opacity=.3] (-4, 1) rectangle (9,-16);
      \fill[white] (-4, 1) rectangle (3, -7.5);
      \fill[blue!25!,opacity=.3] (-4, 1) rectangle (3, -7.5);
      \node[right] at (-4,.5) {\textbf{Interpreter}};
      \node[left] at (9,.5) {\textbf{Translator}};
    \end{scope}

    \node (start) [process, minimum width=2cm] {Start};
    \node (dec1)  [decision, below=of start] {Next region in cache?};
    \node (pro1)  [process, right=of dec1, xshift=3cm] {Translate Region};
    \node (pro2)  [process, below=of dec1] {Execute Translation from cache};
    \node (dec2)  [decision, below=of pro1] {Is optimisation\allowbreak enabled?};
    \node (pro3)  [process, below=of dec2] {Apply peephole\allowbreak optimisations};
    \node (dec3)  [decision, below=of pro3] {Is Koopman optimisation enabled?};
    \node (pro6)  [process, below=of pro2] {Store region in cache};
    \node (pro5)  [process, below=of pro6] {Apply peephole\allowbreak optimisations};
    \node (pro4)  [process, below=of pro5] {Apply stack-scheduling\allowbreak optimisations};
    \coordinate[right=of pro6] (invis);

    \draw [->] (start) -- (dec1);
    \draw [->] (dec1) -- node[anchor=north east] {no} (pro1);
    \draw [->] (dec1) -- node {yes} (pro2);
    \draw [->] (pro1) -- (dec2);
    \draw [->] (dec2) -- node {yes} (pro3);
    \draw [->] (invis) -- (pro6);
    \draw (dec2.south west) -- node[anchor=south east] {no} (invis);
    \draw (dec3.north west) -- node[anchor=north east] {no} (invis);
    \draw [->] (pro3) -- (dec3);
    \draw [->] (dec3) |- node {yes} (pro4);
    \draw [->] (pro4) -- (pro5);
    \draw [->] (pro5) -- (pro6);
    \draw [->] (pro6) -- (pro2);
    \draw [->] ([yshift=3mm] pro2.south east) |- ++(1, 0) |-
    node[yshift=-3mm, anchor=north east] {chain} ([yshift=-3mm] pro2.north east);
    \draw [->] (pro2.west) node[anchor=north east, text width=1.5cm] {no chain} -| (dec1.west);
  \end{tikzpicture}
  \caption{Implementation structure}\label{fig:implementationstructure}
\end{figure}


\section{Test implementations}
Several simple benchmark programs were written in the source DCPU-16
architecture for the purpose of showing the effectiveness of the optimisation
algorithms. First was an implementation of bubble sort that sorted 32 16-bit
numbers, one of the more complex programs, having nested loops. Next was a
Fibonacci program that output the first 20 numbers in the Fibonacci sequence.
This was the simplest program, with a single loop. A prime calculator was next,
which used a prime sieve to find all the prime numbers up to 100. Whilst it had
similarly nested loops like the bubble sort program, it was implemented in a way
such that the loops were actually separated out in the program, so that it
branched a lot more than it would have done otherwise. Lastly there was the
triangle number calculator, which calculated the first 100 triangle numbers
($T_{n} = n(n+1)/2$). The contents of these programs can be seen in
Appendix~\ref{ch:benchmarkprograms}.

For getting results out, the programs would have to be converted several times
with various different commandline flags for the register vs stack conversion
and for the different optimisation levels. For this a simple `test harness' was
written in Python, which looped through all the programs, generating the correct
flags for running them, and ran them, capturing their outputs. First check was
to check the equality of the converted code output with the output of the
standard register emulator. This check would not suffice if both emulators were
incorrect, but in practice checking `by eye' was satisfactory. Then the register
code was converted with each level of optimisation and the output of which is
captured and parsed. The emulator was writing in such a way that at a certain
debug level it would include every instruction that had been run, or information
about instruction caching. After some pattern matching on this output was done,
numerical results were obtained.

