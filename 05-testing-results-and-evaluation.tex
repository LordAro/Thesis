\chapter{Testing, Results and Evaluation}\label{ch:testingresults}
\section{Testing}

Due to the emulated nature of the implementation of this project, testing
focuses on the instructions of the original register-based programs and stack
programs that they produce, at all levels of optimisation --- no optimisations,
peephole optimisations, and stack-scheduling.

The results look to find the relationship between the number of register
instructions to generated instructions, so that the effect of each optimising
level can be assessed. Further attention is paid to the number of memory
accesses made by the stack program, as stack-scheduling is supposed to be able
to significantly reduce redundant memory accesses produced by emulating
registers as memory.

Using a simple memory model, a basic figure for ``program cost'' can be
produced. While there is no equivalent available for the source register-based
programs, it is beneficial to see the difference between each optimisation
level. The memory model is also used to measure the effect of the translation
cache, by giving the conversion and caching of the register code an (estimated)
high cost, to simulate the effects of not having to convert heavily-used loops
many times over.

Testing is done using a series of programs handwritten for the DCPU-16
emulator. For testing purposes, simple test programs (called \texttt{simple},
\texttt{loop} and \texttt{redundant}) were created only to test compiler
correctness and were not used for benchmarking.

Several simple benchmark programs have been written in the source DCPU-16
architecture for the purpose of showing the effectiveness of the optimisation
algorithms. For testing purposes, simple test programs (called \texttt{simple},
\texttt{loop} and \texttt{redundant}) were created only to test compiler
correctness and were not used for benchmarking.

For the actual benchmark programs, there are several desirable features to
consider. Since stack-scheduling is an intra-block algorithm, the benchmark
programs should vary between types of loops that are used, and their frequency
of use. Highly used loops do well to test the translation cache system. The
programs should also use a wide variety of register instructions to achieve
better coverage of the translator and emulators.

A common set of tests used for these type of the programs is the Standford
Benchmark Suite\cite{stanford}. It is a set of programs written in C that are
fairly short in code and execution time. It also has the advantage of not
requiring any input, which both the source DCPU-16 and J5 instruction sets lack.
Examples of programs in it include bubble sort, Tower of Hanoi and a matrix
multiplication.

However, since the implementation in this project lacks any sort of subroutine
or function support, many of the programs would be quite difficult to feasibly
implement. With that in mind, a different set of benchmarks have been created.
Given more time, further benchmarks would have been created so as to produce
more raw data, for better results.

\begin{itemize}[noitemsep]
  \item \texttt{bsort} --- Bubble sort benchmark. Sorts 32 integers and prints
  the result
  \item \texttt{fib20} --- Calculates and prints the first 20 fibonacci numbers
  \item \texttt{primes} --- Uses a prime sieve to get all the prime numbers up
  to 100
  \item \texttt{tri100} --- Outputs the first 100 triangle numbers, using the
    addition formula
\end{itemize}

The bubble sort program, \texttt{bsort}, is an implementation of bubble sort
that sorts 32 16-bit numbers. It is one of the more complex programs with it
having nested loops. A prime calculator, \texttt{primes}, uses a prime sieve to
find all the prime numbers up to 100. Whilst it has similarly nested loops to
\texttt{bsort}, it is implemented in a way such that the loops are logically
separated out in the program, so that it branches a lot more than it would have
done otherwise. A Fibonacci program, \texttt{fib20}, outputs the first 20
numbers in the Fibonacci sequence. This is the simplest benchmark program, only
branching within a single loop. Similarly there is a triangle number calculator,
\texttt{tri100}, that calculates the first 100 triangle numbers
($T_{n} = n(n+1)/2$). The contents of these programs can be seen in
Appendix~\ref{ch:benchmarkprograms}.

\section{Results}

\begin{table}
  \begin{tabular}{r r r r}
                    &   Base & Peephole & Koopman \\ \toprule
    \texttt{bsort}  &  24024 &    22343 &   22233 \\ \midrule
    \texttt{fib20}  &    665 &      608 &     606 \\ \midrule
    \texttt{primes} &   6717 &     6619 &    6619 \\ \midrule
    \texttt{tri100} & 111381 &   106332 &  106332 \\ \midrule
  \end{tabular}
  \caption{No.\ of instructions executed}\label{tab:instructions}
\end{table}

\begin{table}
  \begin{tabular}{r r r r}
                    &  Base & Peephole & Koopman \\ \toprule
    \texttt{bsort}  &  7792 &     6703 &    6065 \\ \midrule
    \texttt{fib20}  &   234 &      196 &     175 \\ \midrule
    \texttt{primes} &  1912 &     1912 &    1643 \\ \midrule
    \texttt{tri100} & 35345 &    35345 &   30296 \\
  \end{tabular}
  \caption{No.\ of LOAD/STORE instructions executed}\label{tab:meminstructions}
\end{table}

\pgfplotstableread[col sep=comma,header=false]{%
bsort,24024,22343,22233
fib20,665,608,606
primes,6717,6619,6619
tri100,111381,106332,106332
}\tracedata%

\pgfplotstableread[col sep=comma,header=false]{%
bsort,7792,6703,6065
fib20,234,196,175
primes,1912,1912,1643
tri100,35345,35345,30296
}\tracememdata%

\pgfplotsset{%
  relative series/.style={%
    table/y expr=\thisrow{#1}/\thisrow{1},
    table/meta=#1
  },
  relative graph/.style={%
    width=13cm,
    ymajorgrids=true,
    major x tick style = transparent,
    major y tick style = transparent,
    scaled y ticks = false,
    ybar=0pt,
    ymin=0.5,
    bar width=0.75cm,
    enlarge x limits=0.2,
    symbolic x coords={bsort,fib20,primes,tri100},
    xtick=data,
    legend cell align=left,
    legend style={%
      at={(1,0)},
      anchor=south west,
      column sep=1ex
    }
  }
}

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[relative graph]
      \addplot table [relative series=1] {\tracedata};
      \addplot table [relative series=2] {\tracedata};
      \addplot table [relative series=3] {\tracedata};
      \legend{Base,Peephole,Koopman}
    \end{axis}
  \end{tikzpicture}
  \caption{Relative no.\ of instructions executed}\label{fig:relativeinstructions}
\end{figure}

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[relative graph]
      \addplot table [relative series=1] {\tracememdata};
      \addplot table [relative series=2] {\tracememdata};
      \addplot table [relative series=3] {\tracememdata};
      \legend{Base,Peephole,Koopman}
    \end{axis}
  \end{tikzpicture}
  \caption{Relative no.\ of LOAD/STORE instructions executed}\label{fig:relativememinstructions}
\end{figure}

Tables~\ref{tab:instructions} and~\ref{tab:meminstructions} represent the number
of instructions executed for a complete run of the benchmark programs, and the
number of LOAD/STORE operations for each of them for the same run, respectively.
Figures~\ref{fig:relativeinstructions} and~\ref{fig:relativememinstructions}
graph the relative decrease in the number of instructions executed for each
optimisation level, where no optimisations is the base level at one.

\begin{table}
\begin{tabular}{l l r r r r}
  Program & Block & Register & Stack & Peephole & Koopman \\ \toprule
  \multirow{6}{*}{\texttt{bsort}}  & & 34 & 103 & 102 & 102 \\
  & \texttt{LOOP}                    &  2 &   9 &   8 &   8 \\
  & \texttt{LOOP2}                   &  5 &  25 &  24 &  24 \\
  & \texttt{SWAP}                    &  3 &  24 &  24 &  23 \\
  & \texttt{POST}                    &  6 &  27 &  25 &  25 \\
  & \texttt{RESL}                    &  4 &  22 &  21 &  21 \\ \midrule
  \multirow{2}{*}{\texttt{fib20}}  & &  6 &  34 &  32 &  32 \\
  & \texttt{LOOP}                    &  7 &  20 &  20 &  18 \\ \midrule
  \multirow{5}{*}{\texttt{primes}} & &  2 &   6 &   6 &   6 \\
  & \texttt{LOOP}                    &  2 &  12 &  12 &  12 \\
  & \texttt{SCANNED}                 &  4 &  17 &  16 &  16 \\
  & \texttt{SCAN}                    &  3 &  17 &  17 &  17 \\
  & \texttt{LOOP2}                   &  4 &  24 &  24 &  24 \\ \midrule
  \multirow{5}{*}{\texttt{tri100}} & &  2 &   6 &   6 &   6 \\
  & \texttt{LOOP}                    &  2 &   6 &   6 &   6 \\
  & \texttt{LOOP2}                   &  4 &  20 &  19 &  19 \\
  & \texttt{BREAK}                   &  4 &  23 &  22 &  22 \\
\end{tabular}
\caption{Instruction counts per block}\label{tab:instructionperblock}
\end{table}

\pgfplotstableread[col sep=comma]{%
Register,Stack,Peephole,Koopman
%34,103,102,102
2,9,8,8
5,25,24,24
3,24,24,23
6,27,25,25
4,22,21,21
6,34,32,32
7,20,20,18
2,6,6,6
2,12,12,12
4,17,16,16
3,17,17,17
4,24,24,24
2,6,6,6
2,6,6,6
4,20,19,19
4,23,22,22
}\blockdata%

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[
        xlabel=No.\ of register instructions,
        ylabel=No.\ of stack instructions,
        legend cell align=left,
        legend style={%
          at={(1,0)},
          anchor=south west,
          column sep=1ex
        }
    ]
      \addplot [only marks,mark=x,blue] table [y=Stack] {\blockdata};
      \addplot [thick,blue] table [y={create col/linear regression={y=Stack}}] {\blockdata};
      \addlegendentry{Base}
      \addplot [only marks,mark=x,red] table [y=Peephole] {\blockdata};
      \addplot [thick,red] table [y={create col/linear regression={y=Peephole}}] {\blockdata};
      \addlegendentry{Peephole}
      \addplot [only marks,mark=x,brown] table [y=Koopman] {\blockdata};
      \addplot [thick,brown] table [y={create col/linear regression={y=Koopman}}] {\blockdata};
      \addlegendentry{Koopman}
      \legend{Base,,Peephole,,Koopman}
    \end{axis}
  \end{tikzpicture}
  \caption{Graph of register blocks to stack equivalents}\label{fig:instructionperblock}
\end{figure}

Table~\ref{tab:instructionperblock} lists the number of stack instructions
produced per block of register code, for different optimisation levels. The
`empty' block row represents the initial block of the program, with no label. It
is graphed in Figure~\ref{fig:instructionperblock}. Notably for the graph, the
initialisation block for \texttt{bsort} is omitted as it is considerably larger
than anything else and considered anomalous.

\begin{table}
\begin{tabular}{l l r r r r}
  Program & Block & Register & Stack & Peephole & Koopman \\ \toprule
  \multirow{6}{*}{\texttt{bsort}}  & & 34 & 35 & 34 & 34 \\
  & \texttt{LOOP}                    &  2 &  3 &  3 &  3 \\
  & \texttt{LOOP2}                   &  5 &  8 &  7 &  6 \\
  & \texttt{SWAP}                    &  3 & 10 & 10 &  9 \\
  & \texttt{POST}                    &  6 &  6 &  5 &  5 \\
  & \texttt{RESL}                    &  4 &  6 &  6 &  5 \\ \midrule
  \multirow{2}{*}{\texttt{fib20}}  & &  6 &  6 &  6 &  4 \\
  & \texttt{LOOP}                    &  7 & 12 & 11 &  9 \\ \midrule
  \multirow{5}{*}{\texttt{primes}} & &  2 &  2 &  6 &  2 \\
  & \texttt{LOOP}                    &  2 &  2 &  2 &  2 \\
  & \texttt{SCANNED}                 &  4 &  4 &  5 &  3 \\
  & \texttt{SCAN}                    &  3 &  5 &  4 &  5 \\
  & \texttt{LOOP2}                   &  4 &  7 &  7 &  6 \\ \midrule
  \multirow{5}{*}{\texttt{tri100}} & &  2 &  2 &  2 &  2 \\
  & \texttt{LOOP}                    &  2 &  2 &  2 &  2 \\
  & \texttt{LOOP2}                   &  4 &  7 &  7 &  6 \\
  & \texttt{BREAK}                   &  4 &  5 &  5 &  4 \\
\end{tabular}
  \caption{LOAD/STORE instruction counts per block}
\label{tab:meminstructionperblock}
\end{table}


\pgfplotstableread[col sep=comma]{%
Register,Stack,Peephole,Koopman
%34,35,34,34
2,3,3,3
5,8,7,6
3,10,10,9
6,6,5,5
4,6,6,5
6,6,6,4
7,12,11,9
2,2,6,2
2,2,2,2
4,4,5,3
3,5,4,5
4,7,7,6
2,2,2,2
2,2,2,2
4,7,7,6
4,5,5,4
}\blockmemdata%

\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[
        xlabel=No.\ of register instructions,
        ylabel=No.\ of LOAD/STORE stack instructions,
        legend cell align=left,
        legend style={%
          at={(1,0)},
          anchor=south west,
          column sep=1ex
        }
    ]
      \addplot [only marks,mark=x,blue] table [y=Stack] {\blockmemdata};
      \addplot [thick,blue] table [y={create col/linear regression={y=Stack}}]
      {\blockmemdata};
      \addlegendentry{Base}
      \addplot [only marks,mark=x,red] table [y=Peephole] {\blockmemdata};
      \addplot [thick,red] table [y={create col/linear regression={y=Peephole}}]
      {\blockmemdata};
      \addlegendentry{Peephole}
      \addplot [only marks,mark=x,brown] table [y=Koopman] {\blockmemdata};
      \addplot [thick,brown] table [y={create col/linear regression={y=Koopman}}]
      {\blockmemdata};
      \addlegendentry{Koopman}
      \legend{Base,,Peephole,,Koopman}
    \end{axis}
  \end{tikzpicture}
  \caption{Graph of register blocks to stack memory read/writes}
\label{fig:meminstructionperblock}
\end{figure}

Table~\ref{tab:meminstructionperblock} lists the number of LOAD/STORE
instructions produced per block of register code, for different optimisation
levels. Similarly to previous tables, the `empty' block row represents the
initial block at the start of the program, given it has no label. The table is
graphed in Figure~\ref{fig:meminstructionperblock} and again excludes the
initialisation block for \texttt{bsort}. Note that the Tables and Figures
compare against the total register instruction count, rather than the number of
memory usages the register blocks have. This is because as registers are being
emulated as memory positions, it would be difficult to reliably determine which
register instructions either use or cause memory usages.

\begin{table}
  \begin{tabular}{l l}
    Caching a block & 10 $\times$ number of register instructions \\
    Retrieving a block & 1 \\
    Memory accesses & 3 \\
    Branches & 2 \\
    Other & 1 \\
  \end{tabular}
  \caption{Memory model used for estimating program cost}\label{tab:memmodel}
\end{table}

\begin{table}
  \begin{tabular}{r r r r}
                    &   Base & Peephole & Koopman \\ \toprule
    \texttt{bsort}  &  97292 &    93746 &   92454 \\ \midrule
    \texttt{fib20}  &   1300 &     1224 &    1142 \\ \midrule
    \texttt{primes} &  19615 &    19516 &   18978 \\ \midrule
    \texttt{tri100} & 202089 &   197040 &  186942 \\ \midrule
  \end{tabular}
  \caption{Program cost with caching disabled}\label{tab:costnocaching}
\end{table}

\begin{table}
  \begin{tabular}{r r r r}
                    &   Base & Peephole & Koopman \\ \toprule
    \texttt{bsort}  &  40902 &    37356 &   36064 \\ \midrule
    \texttt{fib20}  &   1300 &     1224 &    1142 \\ \midrule
    \texttt{primes} &  11981 &    11882 &   11344 \\ \midrule
    \texttt{tri100} & 192583 &   187534 &  177436 \\ \midrule
  \end{tabular}
  \caption{Program cost with caching}\label{tab:costcaching}
\end{table}

\pgfplotstableread[col sep=comma,header=false]{%
bsort,97292,93746,92454,40902,37356,36064
fib20,1300,1224,1142,1300,1224,1142
primes,19615,19516,18978,11981,11882,11344
tri100,202089,197040,186942,192583,187534,177436
}\costdata%

\sloppy\begin{figure}
  \begin{tikzpicture}
    \begin{axis}[relative graph,bar width=0.25cm,ymin=0,width=11.5cm]
      \addplot table [relative series=1] {\costdata};
      \addplot table [relative series=4] {\costdata};
      \addplot table [relative series=2] {\costdata};
      \addplot table [relative series=5] {\costdata};
      \addplot table [relative series=3] {\costdata};
      \addplot table [relative series=6] {\costdata};
      \legend{Base,Base Cached,Peephole,Peephole Cached,Koopman,Koopman Cached}
    \end{axis}
  \end{tikzpicture}
  \caption{Relative program cost with and without caching}\label{fig:progcost}
\end{figure}

Tables~\ref{tab:costnocaching} and~\ref{tab:costcaching} describe the `costs' of
running the benchmarking programs with the memory model described in
Table~\ref{tab:memmodel}. The two tables are represented in
Figure~\ref{fig:progcost}.

\section{Evaluation}\label{sec:testingevaluation}

The basic results (seen in Table~\ref{tab:instructions} and
Figure~\ref{fig:relativeinstructions}) are somewhat counter-intuative, showing
that the peephole optimisation stage does the bulk of the the overall executed
instruction count reduction and that Koopman-style stack scheduling has very
little effect.However, looking at the number of memory reads and writes (with
the `\texttt{LOAD}' and `\texttt{STORE}' instructions respectively) in
Table~\ref{tab:meminstructions} (and visualised in
Figure~\ref{fig:relativememinstructions}) shows a different result, with stack
scheduling doing as much reduction as peephole optimisation if not more,
particularly in the case of the \texttt{primes} and \texttt{tri100} programs
which have no instruction decrease with stack scheduling, but a significant
decrease in memory usage.

While the optimisation results are also good (around 30\% reduction in memory
usage is quite substantial), making it appear as though they are not the same as
what Koopman~\cite{Koopman1995Preliminary} achieved in his original paper, being
able to remove 90--100\% of redundant local variable accesses. The results here
are not exactly the same measurement, but it seems reasonable to assume them to
be comparable. The actual number of redundant memory accesses is quite hard to
programatically calculate, and Koopman did it by studying the program's output
by hand. A reasonable explanation is that the limited set of benchmark programs
prevents a statistically significant result and other benchmark programs would
have more opportunities for stack scheduling than the programs presented here.
The programs used here have fairly short loops or regions (in number of
instructions) naturally making lots of reuse of variables less likely than
otherwise.

Looking at Table~\ref{tab:instructionperblock} and its linked
Figure~\ref{fig:instructionperblock}, it can be seen (per block) that while
stack-scheduling does have an effect on instruction count, it is the preliminary
peephole optimisations that have the majority of the effect. Despite this,
looking at Table~\ref{tab:meminstructionperblock} and
Figure~\ref{fig:meminstructionperblock} again shows where stack-scheduling
becomes relevant and useful with the reduction in memory usage, where the linear
regression line shows significantly lower average memory usage than peephole and
no optimisations. The linear regression line for peephole optimisations starts
above the base stack output, this is likely due to the small size of the data
set --- overall the line's gradient is shallower than the base output, implying
that the results would continue to improve for larger block sizes.

Figures~\ref{fig:instructionperblock} and~\ref{fig:meminstructionperblock} also
show the relationship of register instructions to stack instructions quite well,
with one register instruction generating between 4 and 5 stack instructions
depending on the optimisation level. The memory accesses are even more
interesting, with Koopman's stack scheduling getting the ratio of register
instructions to memory accesses down to just over one. It is reasonable to
conclude then that the stack-scheduling does in fact remove many of the
unnecessary memory accesses. Contrary to the conclusions of
Figures~\ref{fig:relativeinstructions} and~\ref{fig:relativememinstructions} as
presented above, this seems to go much further to validate Koopman's claim of
near total removal of redundant memory accesses as it shows that register
machines are not inherently any better than stack machines in terms of the
overhead of fetching data from memory or registers compared to memory or the
stack.

The effects of the instruction snippet cache are more impressive. As seen
Figure~\ref{fig:progcost} (with raw data in Tables~\ref{tab:costnocaching}
and~\ref{tab:costcaching}), each optimisation level decreases cost in a similar
fashion to the raw instruction counts discussed earlier. However, caching these
instruction blocks leads to some significant decreases in program cost for
highly looping programs, like \texttt{bsort} and \texttt{primes}. Caching
notably has no effect on \texttt{fib20} and hardly any on \texttt{tri100} ---
this is down to the simplicity of the programs involved as they make very little
use of branching to change to a different block, only branching within the block
is used. While the costs of each instruction are only estimates
(Table~\ref{tab:memmodel}), it shows the potential effectiveness of the
instruction caching well. The estimated cost of converting register instructions
is high compared to reality, given the model's unconditional cost of instruction
translation (at 10x number of instructions) which would vary more in practice,
so further improvements may be available depending on its actual efficiency.

Overall, this is a very positive result, as while the generated stack programs a
lot larger in instruction count, the memory usages are approximately equivalent.
Which when taking into account how a stack machine would be simpler than a
similar register machine and so would be easier to construct and potentially run
at a higher clock speed, making the machines even more equivalent in program
execution cost. A reasonable continuation of this would be to alter the stack
assembly language to help remove some common repeated instruction groups. For
example, for the J5 assembly language, it would be useful to have a
\texttt{LOAD} instruction that actually took a memory address, and similarly for
the \texttt{STORE}, to save having to hardcode a separate \texttt{SET}
instruction to put a value onto the stack for each instruction. A further
advanced step would be to vary the produced stack code depending on its
contents, i.e.\ if a particular group of register instructions could be replaced
with something simpler than the `standard' generated code. A simple example of
that in this project is the peephole optimisation done on converting \texttt{SET
1} and \texttt{ADD} instructions to an \texttt{INC} instruction. It is not done
at the initial translation and the register instruction set lacks a similar
instruction.

\subsection{Completion of requirements}

All key features of the project have been completed, but only one of the
desirable features has been able to be partially implemented, largely due to
time constraints.

Requirements~\ref{itm:register} and~\ref{itm:stack} (implementing the emulators)
have both been fully implemented, with working parsers for their respective
assembly languages and able to interpret and execute any program in that
language, with a fully implemented instruction set. Code generation is also
fully implemented (Requirement~\ref{itm:generate}) that was free of side
effects. The two key optimisation methods (Requirement~\ref{itm:optimise}) have
also been implemented, including commandline flags to enable/disable them as
required (similarly to optimise flags in C++ compilers) These flags have been
used to get several sets of results (Requirement~\ref{itm:results}) focusing on
length of generated code with each optimisation level and the amount of memory
accesses.

Unfortunately, the additional requirements (notably
Requirements~\ref{itm:moreoptimise} and~\ref{itm:visualise}) have not been able
to be implemented, largely due to time constraints on the project. While the
tool does have various debugging levels with which to get output out of, the
interface is rather crude and the data output needs some transformations (using
a Python testing interface and \texttt{grep}) to get meaningful results out of,
so it would be preferrable to have a user interface for collecting data from.
However, there has been partial completion of Requirement~\ref{itm:caching}, in
that each converted block is cached, but there is no concept of a `threshold' at
which to convert them. This would have been useful as the expense of converting
and caching the initalisation routines (that set all the input data) is likely
too much compared to just running them, bringing up the `program cost' of the
program as discussed in the results above. It would be preferable to only
convert and cache loops that are executed lots of times.

\subsection{Program correctness}
No significant automated testing of the implementation has been done. Instead
the program's output is only verified `by eye' and for the test run the multiple
executions are compared against each other, to ensure consistent results.

\subsection{Code quality}
The code itself is fairly extensible for future work. It is published as
open-source software at \texttt{github.com/LordAro/reg2stack} enabling any
future work to be able to be built on top of this software. The peephole
optimisation implementation allows for easy additions --- just add the relevant
function and add the function name to the existing list of functions.

Documentation of the code could definitely be improved. While the complicated
sections of code generally has comments explaining exactly why it does what it
does, it is by no means complete and more code could benefit from comments. Many
functions are also missing `doc-comments', which generally take the form of
comments above the function definition which can then be parsed by some external
tool to generate more formal documentation. If this were expanded, it would be
really helpful to future work on the program.
